{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d669891",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "import re\n",
    "\n",
    "import open3d as o3d\n",
    "import numpy as np\n",
    "from pyntcloud import PyntCloud\n",
    "from multiprocessing import Pool\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe19eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some of the functions are taken from pykitti https://github.com/utiasSTARS/pykitti/blob/master/pykitti/utils.py\n",
    "def load_velo_scan(file):\n",
    "    \"\"\"\n",
    "    Load and parse a velodyne binary file\n",
    "    \"\"\"\n",
    "    scan = np.fromfile(file, dtype=np.float32)\n",
    "    return scan.reshape((-1, 4))\n",
    "\n",
    "def load_poses(file):\n",
    "    \"\"\"\n",
    "    Load and parse ground truth poses\n",
    "    \"\"\"\n",
    "    tmp_poses = np.genfromtxt(file, delimiter=' ').reshape(-1, 3, 4)\n",
    "    poses = np.repeat(np.expand_dims(np.eye(4), 0), tmp_poses.shape[0], axis=0)\n",
    "    poses[:, 0:3, :] = tmp_poses\n",
    "    return poses\n",
    "\n",
    "def read_calib_file(filepath):\n",
    "    \"\"\"\n",
    "    Read in a calibration file and parse into a dictionary\n",
    "    \"\"\"\n",
    "    data = {}\n",
    "\n",
    "    with open(filepath, 'r') as f:\n",
    "        for line in f.readlines():\n",
    "            key, value = line.split(':', 1)\n",
    "            try:\n",
    "                data[key] = np.array([float(x) for x in value.split()])\n",
    "            except ValueError:\n",
    "                pass\n",
    "\n",
    "    return data\n",
    "\n",
    "# This part of the code is taken from the semanticKITTI API\n",
    "def open_label(filename):\n",
    "    \"\"\" \n",
    "    Open raw scan and fill in attributes\n",
    "    \"\"\"\n",
    "    # check filename is string\n",
    "    if not isinstance(filename, str):\n",
    "        raise TypeError(\"Filename should be string type, \"\n",
    "                        \"but was {type}\".format(type=str(type(filename))))\n",
    "\n",
    "    # if all goes well, open label\n",
    "    label = np.fromfile(filename, dtype=np.uint32)\n",
    "    label = label.reshape((-1))\n",
    "\n",
    "    return label\n",
    "\n",
    "def set_label(label, points):\n",
    "    \"\"\" \n",
    "    Set points for label not from file but from np\n",
    "    \"\"\"\n",
    "    # check label makes sense\n",
    "    if not isinstance(label, np.ndarray):\n",
    "        raise TypeError(\"Label should be numpy array\")\n",
    "\n",
    "    # only fill in attribute if the right size\n",
    "    if label.shape[0] == points.shape[0]:\n",
    "        sem_label = label & 0xFFFF  # semantic label in lower half\n",
    "        inst_label = label >> 16    # instance id in upper half\n",
    "    else:\n",
    "        print(\"Points shape: \", points.shape)\n",
    "        print(\"Label shape: \", label.shape)\n",
    "        raise ValueError(\"Scan and Label don't contain same number of points\")\n",
    "\n",
    "    # sanity check\n",
    "    assert((sem_label + (inst_label << 16) == label).all())\n",
    "\n",
    "    return sem_label, inst_label\n",
    "\n",
    "def transform_point_cloud(x1, R, t):\n",
    "    \"\"\"\n",
    "    Transforms the point cloud using the giver transformation paramaters\n",
    "    \n",
    "    Args:\n",
    "        x1  (np array): points of the point cloud [n,3]\n",
    "        R   (np array): estimated rotation matrice [3,3]\n",
    "        t   (np array): estimated translation vectors [3,1]\n",
    "    Returns:\n",
    "        x1_t (np array): points of the transformed point clouds [n,3]\n",
    "    \"\"\"\n",
    "    x1_t = (np.matmul(R, x1.transpose()) + t).transpose()\n",
    "\n",
    "    return x1_t\n",
    "\n",
    "def sorted_alphanum(file_list_ordered):\n",
    "    \"\"\"\n",
    "    Sorts the list alphanumerically\n",
    "    Args:\n",
    "        file_list_ordered (list): list of files to be sorted\n",
    "    Return:\n",
    "        sorted_list (list): input list sorted alphanumerically\n",
    "    \"\"\"\n",
    "    def convert(text):\n",
    "        return int(text) if text.isdigit() else text\n",
    "\n",
    "    def alphanum_key(key):\n",
    "        return [convert(c) for c in re.split('([0-9]+)', key)]\n",
    "\n",
    "    sorted_list = sorted(file_list_ordered, key=alphanum_key)\n",
    "\n",
    "    return sorted_list\n",
    "\n",
    "def get_file_list(path, extension=None):\n",
    "    \"\"\"\n",
    "    Build a list of all the files in the provided path\n",
    "    Args:\n",
    "        path (str): path to the directory \n",
    "        extension (str): only return files with this extension\n",
    "    Return:\n",
    "        file_list (list): list of all the files (with the provided extension) sorted alphanumerically\n",
    "    \"\"\"\n",
    "    if extension is None:\n",
    "        file_list = [os.path.join(path, f) for f in os.listdir(path) if os.path.isfile(os.path.join(path, f))]\n",
    "    else:\n",
    "        file_list = [\n",
    "            os.path.join(path, f)\n",
    "            for f in os.listdir(path)\n",
    "            if os.path.isfile(os.path.join(path, f)) and os.path.splitext(f)[1] == extension\n",
    "        ]\n",
    "    file_list = sorted_alphanum(file_list)\n",
    "\n",
    "    return file_list\n",
    "\n",
    "\n",
    "def get_folder_list(path):\n",
    "    \"\"\"\n",
    "    Build a list of all the folders in the provided path\n",
    "    Args:\n",
    "        path (str): path to the directory \n",
    "    Returns:\n",
    "        folder_list (list): list of all the folders sorted alphanumerically\n",
    "    \"\"\"\n",
    "    folder_list = [os.path.join(path, f) for f in os.listdir(path) if os.path.isdir(os.path.join(path, f))]\n",
    "    folder_list = sorted_alphanum(folder_list)\n",
    "    \n",
    "    return folder_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8c8c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_consecutive_point_cloud(pc_s, R1, t1, R2, t2):\n",
    "    pc = np.matmul(np.linalg.inv(R2), (np.matmul(R1, pc_s.transpose()) + t1) - t2).transpose()\n",
    "    return pc\n",
    "\n",
    "def get_eigen_features(pc, n_neighbors=48):\n",
    "    \"\"\"\n",
    "    Args: \n",
    "        pc (np array): points of the point cloud [n, 3]\n",
    "        n_neighbors (int): number of neighbors selected\n",
    "    Returns: \n",
    "        pc_feature (np array): features of the point cloud [n, 14]\n",
    "    \"\"\"\n",
    "    \n",
    "    pcd = o3d.geometry.PointCloud()\n",
    "    pcd.points = o3d.utility.Vector3dVector(pc)\n",
    "    pointcloud = PyntCloud.from_instance(\"open3d\", pcd)\n",
    "\n",
    "    neighbors = pointcloud.get_neighbors(k=n_neighbors)\n",
    "    eigenvalues = pointcloud.add_scalar_field(\"eigen_values\", k_neighbors=neighbors)\n",
    "\n",
    "    anisotropy = pointcloud.add_scalar_field(\"anisotropy\", ev=eigenvalues)\n",
    "    curvature = pointcloud.add_scalar_field(\"curvature\", ev=eigenvalues)\n",
    "    eigenentropy = pointcloud.add_scalar_field(\"eigenentropy\", ev=eigenvalues)\n",
    "    eigensum = pointcloud.add_scalar_field(\"eigen_sum\", ev=eigenvalues)\n",
    "    linearity = pointcloud.add_scalar_field(\"linearity\", ev=eigenvalues)\n",
    "    omnivariance = pointcloud.add_scalar_field(\"omnivariance\", ev=eigenvalues)\n",
    "    planarity = pointcloud.add_scalar_field(\"planarity\", ev=eigenvalues)\n",
    "    sphericity = pointcloud.add_scalar_field(\"sphericity\", ev=eigenvalues)\n",
    "\n",
    "    pc_feature = np.asarray(pointcloud.points)\n",
    "\n",
    "    return pc_feature\n",
    "\n",
    "def cal_chamfer_dist(source_pts, target_pts, length):\n",
    "    if(target_pts.size == 0):\n",
    "        return 24 * length * length\n",
    "    source_pcd = o3d.geometry.PointCloud()\n",
    "    source_pcd.points = o3d.utility.Vector3dVector(source_pts[:, 0:3].reshape(-1, 3))\n",
    "    target_pcd = o3d.geometry.PointCloud()\n",
    "    target_pcd.points = o3d.utility.Vector3dVector(target_pts[:, 0:3].reshape(-1, 3))\n",
    "    \n",
    "    source2target_dists = source_pcd.compute_point_cloud_distance(target_pcd)\n",
    "    target2source_dists = target_pcd.compute_point_cloud_distance(source_pcd)\n",
    "    source2target_dists = np.asarray(source2target_dists)\n",
    "    target2source_dists = np.asarray(target2source_dists)\n",
    "    source2target_dists = np.square(source2target_dists)\n",
    "    target2source_dists = np.square(target2source_dists)\n",
    "    chamfer_dist = source2target_dists.mean() + target2source_dists.mean()\n",
    "    \n",
    "    return chamfer_dist\n",
    "\n",
    "def find_neighbors(voxel_center, source_pts, target_pts, length):\n",
    "    source_idx = np.where((source_pts[:,0] > voxel_center[0] - length) & (source_pts[:,0] < voxel_center[0] + length) & \n",
    "                          (source_pts[:,1] > voxel_center[1] - length) & (source_pts[:,1] < voxel_center[1] + length) & \n",
    "                          (source_pts[:,2] > voxel_center[2] - length) & (source_pts[:,2] < voxel_center[2] + length))[0]\n",
    "    target_idx = np.where((target_pts[:,0] > voxel_center[0] - length) & (target_pts[:,0] < voxel_center[0] + length) & \n",
    "                          (target_pts[:,1] > voxel_center[1] - length) & (target_pts[:,1] < voxel_center[1] + length) & \n",
    "                          (target_pts[:,2] > voxel_center[2] - length) & (target_pts[:,2] < voxel_center[2] + length))[0]\n",
    "    \n",
    "    source_pts = source_pts[source_idx, :]\n",
    "    target_pts = target_pts[target_idx, :]\n",
    "    \n",
    "    return source_pts, target_pts, source_idx\n",
    "\n",
    "def chamfer_dist(source_pts, target_pts, length=1):\n",
    "    pcd = o3d.geometry.PointCloud()\n",
    "    pcd.points = o3d.utility.Vector3dVector(source_pts)\n",
    "    voxel_grid = o3d.geometry.VoxelGrid.create_from_point_cloud(pcd, voxel_size=length * 2)\n",
    "    voxels_all = voxel_grid.get_voxels()\n",
    "    \n",
    "    chamfer = np.ones(source_pts.shape[0]) * -1\n",
    "    \n",
    "    for voxel in voxels_all:\n",
    "        voxel_center = voxel_grid.get_voxel_center_coordinate(voxel.grid_index)\n",
    "        source_neighbors_pts, target_neighbors_pts, source_neighbors_idx = find_neighbors(voxel_center, source_pts, target_pts, length)\n",
    "        chamfer[source_neighbors_idx] = cal_chamfer_dist(source_neighbors_pts, target_neighbors_pts, length)\n",
    "    \n",
    "    return chamfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e932c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class semanticKITTIProcesor:\n",
    "    def __init__(self, raw_data_path, save_path, save_ply, save_near, n_processes):\n",
    "        self.root_path = raw_data_path\n",
    "        self.save_path = save_path\n",
    "        self.save_ply = save_ply\n",
    "        self.save_near = save_near\n",
    "        self.n_processes = n_processes\n",
    "\n",
    "        self.scenes = get_folder_list(self.root_path)\n",
    "\n",
    "    def run_processing(self):\n",
    "        if self.n_processes < 1:\n",
    "            self.n_processes = 1\n",
    "\n",
    "        pool = Pool(self.n_processes)\n",
    "        pool.map(self.process_scene, self.scenes)\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "\n",
    "    def process_scene(self, scene):\n",
    "        scene_name = scene.split(os.sep)[-1]\n",
    "\n",
    "        # Create a save file if not existing\n",
    "        if not os.path.exists(os.path.join(self.save_path, scene_name)):\n",
    "            os.makedirs(os.path.join(self.save_path, scene_name))\n",
    "        \n",
    "        # Load transformation paramters\n",
    "        poses = load_poses(os.path.join(scene, 'poses.txt'))\n",
    "        tr_velo_cam = read_calib_file(os.path.join(scene, 'calib.txt'))['Tr'].reshape(3, 4)\n",
    "        tr_velo_cam = np.concatenate((tr_velo_cam, np.array([0, 0, 0, 1]).reshape(1, 4)), axis=0)\n",
    "        frames = get_file_list(os.path.join(scene, 'velodyne'), extension='.bin')\n",
    "\n",
    "        if os.path.isdir(os.path.join(scene,'labels')):\n",
    "            labels = get_file_list(os.path.join(scene,'labels'), extension='.label')\n",
    "            test_scene = False\n",
    "                    \n",
    "            assert len(frames) == len(labels), \"Number of point cloud fils and label files is not the same!\"\n",
    "        \n",
    "        else:\n",
    "            test_scene = True\n",
    "        \n",
    "        # Operate on each time frame \n",
    "        for idx in range(len(frames) - 1):\n",
    "            print(idx)\n",
    "            frame_name_s = frames[idx].split(os.sep)[-1].split('.')[0]\n",
    "            frame_name_t = frames[idx + 1].split(os.sep)[-1].split('.')[0]\n",
    "\n",
    "            pc_s = load_velo_scan(frames[idx])[:, :3]\n",
    "            pc_t = load_velo_scan(frames[idx + 1])[:, :3]\n",
    "\n",
    "            # Transform both point cloud to the camera coordinate system (check KITTI webpage)\n",
    "            pc_s = transform_point_cloud(pc_s, tr_velo_cam[:3, :3], tr_velo_cam[:3, 3:4])\n",
    "            pc_t = transform_point_cloud(pc_t, tr_velo_cam[:3, :3], tr_velo_cam[:3, 3:4])\n",
    "            \n",
    "            # Transform the source point cloud from original coordinates to the target point cloud's coordinates\n",
    "            R_s = poses[idx, 0:3, 0:3].reshape(3, 3)\n",
    "            t_s = poses[idx, 0:3, 3].reshape(3, 1)\n",
    "            R_t = poses[idx + 1, 0:3, 0:3].reshape(3, 3)\n",
    "            t_t = poses[idx + 1, 0:3, 3].reshape(3, 1)\n",
    "            \n",
    "            pc_s = match_consecutive_point_cloud(pc_s, R_s, t_s, R_t, t_t)\n",
    "            \n",
    "            # Rotate 180 degrees around z axis (to be in accordance to KITTI flow as used by other datsets)\n",
    "            pc_s[:, 0], pc_s[:, 1] = -pc_s[:, 0], -pc_s[:, 1]\n",
    "            pc_t[:, 0], pc_t[:, 1] = -pc_t[:, 0], -pc_t[:, 1]\n",
    "                \n",
    "            # Extract eigen features\n",
    "            eigen_features_s = get_eigen_features(pc_s)\n",
    "            eigen_features_t = get_eigen_features(pc_t)\n",
    "\n",
    "            if not test_scene:\n",
    "                # Load the labels\n",
    "                sem_label_s, inst_label_s = set_label(open_label(labels[idx]), pc_s)\n",
    "                sem_label_t, inst_label_t = set_label(open_label(labels[idx + 1]), pc_t)\n",
    "                \n",
    "                # Remove ground points by naively thresholding the vertical coordinate\n",
    "                # ground_mask_s = pc_s[:, 1] > -1.4\n",
    "                # ground_mask_t = pc_t[:, 1] > -1.4\n",
    "                # pc_s = pc_s[ground_mask_s, :]\n",
    "                # pc_t = pc_t[ground_mask_t, :]\n",
    "                \n",
    "                # sem_label_s = sem_label_s[ground_mask_s]\n",
    "                # inst_label_s = inst_label_s[ground_mask_s]\n",
    "\n",
    "                # sem_label_t = sem_label_t[ground_mask_t]\n",
    "                # inst_label_t = inst_label_t[ground_mask_t]\n",
    "                \n",
    "                # eigen_features_s = eigen_features_s[ground_mask_s]\n",
    "                # eigen_features_t = eigen_features_t[ground_mask_t]\n",
    "                \n",
    "                class_mask_s = np.where((sem_label_s != 0) & (sem_label_s != 1) & (sem_label_s != 40) & (sem_label_s != 44) & (sem_label_s != 48) & (sem_label_s != 49) & (sem_label_s != 60) & (sem_label_s != 72))[0]\n",
    "                class_mask_t = np.where((sem_label_t != 0) & (sem_label_t != 1) & (sem_label_t != 40) & (sem_label_t != 44) & (sem_label_t != 48) & (sem_label_t != 49) & (sem_label_t != 60) & (sem_label_t != 72))[0]\n",
    "                pc_s = pc_s[class_mask_s, :]\n",
    "                pc_t = pc_t[class_mask_t, :]\n",
    "\n",
    "                sem_label_s = sem_label_s[class_mask_s]\n",
    "                inst_label_s = inst_label_s[class_mask_s]\n",
    "\n",
    "                sem_label_t = sem_label_t[class_mask_t]\n",
    "                inst_label_t = inst_label_t[class_mask_t]\n",
    "                \n",
    "                eigen_features_s = eigen_features_s[class_mask_s]\n",
    "                eigen_features_t = eigen_features_t[class_mask_t]\n",
    "                \n",
    "                # Remove points which are behind the car (to be in accordance with the stereo datasets)\n",
    "                front_mask_s = pc_s[:, 2] > 1.5\n",
    "                front_mask_t = pc_t[:, 2] > 1.5\n",
    "                pc_s = pc_s[front_mask_s, :]\n",
    "                pc_t = pc_t[front_mask_t, :]\n",
    "\n",
    "                sem_label_s = sem_label_s[front_mask_s]\n",
    "                inst_label_s = inst_label_s[front_mask_s]\n",
    "\n",
    "                sem_label_t = sem_label_t[front_mask_t]\n",
    "                inst_label_t = inst_label_t[front_mask_t]\n",
    "                \n",
    "                eigen_features_s = eigen_features_s[front_mask_s]\n",
    "                eigen_features_t = eigen_features_t[front_mask_t]\n",
    "\n",
    "                # Remove points whose depth is more than 35m to prevent depth values from explosion\n",
    "                if self.save_near:\n",
    "                    near_mask_s = pc_s[:, 2] < 35\n",
    "                    near_mask_t = pc_t[:, 2] < 35\n",
    "                    pc_s = pc_s[near_mask_s, :]\n",
    "                    pc_t = pc_t[near_mask_t, :] \n",
    "\n",
    "                    sem_label_s = sem_label_s[near_mask_s]\n",
    "                    inst_label_s = inst_label_s[near_mask_s]\n",
    "\n",
    "                    sem_label_t = sem_label_t[near_mask_t]\n",
    "                    inst_label_t = inst_label_t[near_mask_t]\n",
    "                    \n",
    "                    eigen_features_s = eigen_features_s[near_mask_s]\n",
    "                    eigen_features_t = eigen_features_t[near_mask_t]\n",
    "                \n",
    "                # Extract static objects\n",
    "                # Dynamic labels are 1 if moving and 0 if static\n",
    "                static_idx_s = np.where(sem_label_s < 100)[0]\n",
    "                static_idx_t = np.where(sem_label_t < 100)[0]\n",
    "                dynamic_label_s = np.ones_like(sem_label_s)\n",
    "                dynamic_label_s[static_idx_s] = 0\n",
    "\n",
    "                dynamic_label_t = np.ones_like(sem_label_t)\n",
    "                dynamic_label_t[static_idx_t] = 0\n",
    "                \n",
    "                # Calculate Chamfer distance\n",
    "                dist = np.array(chamfer_dist(pc_s, pc_t))\n",
    "\n",
    "                np.savez_compressed(os.path.join(self.save_path, scene_name, '{}_{}.npz'.format(frame_name_s, frame_name_t)),\n",
    "                                    pc1=pc_s,\n",
    "                                    pc2=pc_t,\n",
    "                                    eigen_features_s = eigen_features_s,\n",
    "                                    eigen_features_t = eigen_features_t,\n",
    "                                    sem_label_s=sem_label_s,\n",
    "                                    inst_label_s=inst_label_s,\n",
    "                                    dynamic_label_s=dynamic_label_s,\n",
    "                                    chamfer_dist=dist)\n",
    "            else:\n",
    "                # Remove ground points by naively thresholding the vertical coordinate\n",
    "                ground_mask_s = pc_s[:, 1] > -1.4\n",
    "                ground_mask_t = pc_t[:, 1] > -1.4\n",
    "                pc_s = pc_s[ground_mask_s, :]\n",
    "                pc_t = pc_t[ground_mask_t, :]\n",
    "                \n",
    "                eigen_features_s = eigen_features_s[ground_mask_s]\n",
    "                eigen_features_t = eigen_features_t[ground_mask_t]\n",
    "                \n",
    "                # Remove points which are behind the car (to be in accordance with the stereo datasets)\n",
    "                front_mask_s = pc_s[:, 2] > 1.5\n",
    "                front_mask_t = pc_t[:, 2] > 1.5\n",
    "                pc_s = pc_s[front_mask_s, :]\n",
    "                pc_t = pc_t[front_mask_t,:]\n",
    "                \n",
    "                eigen_features_s = eigen_features_s[front_mask_s]\n",
    "                eigen_features_t = eigen_features_t[front_mask_t]\n",
    "\n",
    "                # Remove points whose depth is more than 30m to prevent depth values from explosion\n",
    "                if self.save_near:\n",
    "                    near_mask_s = pc_s[:, 2] < 35\n",
    "                    near_mask_t = pc_t[:, 2] < 35\n",
    "                    pc_s = pc_s[near_mask_s, :]\n",
    "                    pc_t = pc_t[near_mask_t, :]\n",
    "                    \n",
    "                    eigen_features_s = eigen_features_s[near_mask_s]\n",
    "                    eigen_features_t = eigen_features_t[near_mask_t]\n",
    "                    \n",
    "                # Calculate Chamfer distance\n",
    "                dist = np.array(chamfer_dist(pc_s, pc_t))\n",
    "\n",
    "                np.savez_compressed(os.path.join(self.save_path, scene_name, '{}_{}.npz'.format(frame_name_s, frame_name_t)),\n",
    "                                    pc1=pc_s,\n",
    "                                    pc2=pc_t,\n",
    "                                    eigen_features_s = eigen_features_s,\n",
    "                                    eigen_features_t = eigen_features_t,\n",
    "                                    chamfer_dist=dist)\n",
    "            \n",
    "            # Save point clouds as ply files\n",
    "            if self.save_ply:\n",
    "                pcd_s = o3d.geometry.PointCloud()\n",
    "                pcd_t = o3d.geometry.PointCloud()\n",
    "                pcd_s.points = o3d.utility.Vector3dVector(pc_s)\n",
    "                pcd_t.points = o3d.utility.Vector3dVector(pc_t)\n",
    "\n",
    "                o3d.io.write_point_cloud(os.path.join(self.save_path, scene_name, '{}.ply'.format(frame_name_s)), pcd_s)\n",
    "                o3d.io.write_point_cloud(os.path.join(self.save_path, scene_name, '{}.ply'.format(frame_name_t)), pcd_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b1413b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__': \n",
    "    processor = semanticKITTIProcesor(raw_data_path = \"\\\\Workspace\\\\Odometry\\\\dataset\\\\KITTI\\\\sequences\\\\00\",\n",
    "                                      save_path = \"\\\\Workspace\\\\SceneFlow\\\\datasets\\\\training_dataset\",\n",
    "                                      save_ply = False,\n",
    "                                      save_near = True,\n",
    "                                      n_processes = 16)\n",
    "    # processor.process_scene(\"\\\\Workspace\\\\Odometry\\\\dataset\\\\KITTI\\\\sequences\\\\00\")\n",
    "    processor.run_processing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7148b553",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
