{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b25bad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pickle\n",
    "\n",
    "import open3d as o3d\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cluster import DBSCAN\n",
    "from pyntcloud import PyntCloud\n",
    "import xgboost as xgb\n",
    "import threading\n",
    "import rpointhop\n",
    "import utils\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2114284",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some of the functions are taken from pykitti https://github.com/utiasSTARS/pykitti/blob/master/pykitti/utils.py\n",
    "def load_velo_scan(file):\n",
    "    \"\"\"\n",
    "    Load and parse a velodyne binary file\n",
    "    \"\"\"\n",
    "    scan = np.fromfile(file, dtype=np.float32)\n",
    "    return scan.reshape((-1, 4))\n",
    "\n",
    "def load_poses(file):\n",
    "    \"\"\"\n",
    "    Load and parse ground truth poses\n",
    "    \"\"\"\n",
    "    tmp_poses = np.genfromtxt(file, delimiter=' ').reshape(-1, 3, 4)\n",
    "    poses = np.repeat(np.expand_dims(np.eye(4), 0), tmp_poses.shape[0], axis=0)\n",
    "    poses[:, 0:3, :] = tmp_poses\n",
    "    return poses\n",
    "\n",
    "def read_calib_file(filepath):\n",
    "    \"\"\"\n",
    "    Read in a calibration file and parse into a dictionary\n",
    "    \"\"\"\n",
    "    data = {}\n",
    "\n",
    "    with open(filepath, 'r') as f:\n",
    "        for line in f.readlines():\n",
    "            key, value = line.split(':', 1)\n",
    "            try:\n",
    "                data[key] = np.array([float(x) for x in value.split()])\n",
    "            except ValueError:\n",
    "                pass\n",
    "\n",
    "    return data\n",
    "\n",
    "# This part of the code is taken from the semanticKITTI API\n",
    "def open_label(filename):\n",
    "    \"\"\" \n",
    "    Open raw scan and fill in attributes\n",
    "    \"\"\"\n",
    "    # check filename is string\n",
    "    if not isinstance(filename, str):\n",
    "        raise TypeError(\"Filename should be string type, \"\n",
    "                        \"but was {type}\".format(type=str(type(filename))))\n",
    "\n",
    "    # if all goes well, open label\n",
    "    label = np.fromfile(filename, dtype=np.uint32)\n",
    "    label = label.reshape((-1))\n",
    "\n",
    "    return label\n",
    "\n",
    "def set_label(label, points):\n",
    "    \"\"\" \n",
    "    Set points for label not from file but from np\n",
    "    \"\"\"\n",
    "    # check label makes sense\n",
    "    if not isinstance(label, np.ndarray):\n",
    "        raise TypeError(\"Label should be numpy array\")\n",
    "\n",
    "    # only fill in attribute if the right size\n",
    "    if label.shape[0] == points.shape[0]:\n",
    "        sem_label = label & 0xFFFF  # semantic label in lower half\n",
    "        inst_label = label >> 16    # instance id in upper half\n",
    "    else:\n",
    "        print(\"Points shape: \", points.shape)\n",
    "        print(\"Label shape: \", label.shape)\n",
    "        raise ValueError(\"Scan and Label don't contain same number of points\")\n",
    "\n",
    "    # sanity check\n",
    "    assert((sem_label + (inst_label << 16) == label).all())\n",
    "\n",
    "    return sem_label, inst_label\n",
    "\n",
    "def transform_point_cloud(x1, R, t):\n",
    "    \"\"\"\n",
    "    Transforms the point cloud using the giver transformation paramaters\n",
    "    \n",
    "    Args:\n",
    "        x1  (np array): points of the point cloud [n,3]\n",
    "        R   (np array): estimated rotation matrice [3,3]\n",
    "        t   (np array): estimated translation vectors [3,1]\n",
    "    Returns:\n",
    "        x1_t (np array): points of the transformed point clouds [n,3]\n",
    "    \"\"\"\n",
    "    x1_t = (np.matmul(R, x1.transpose()) + t).transpose()\n",
    "\n",
    "    return x1_t\n",
    "\n",
    "def sorted_alphanum(file_list_ordered):\n",
    "    \"\"\"\n",
    "    Sorts the list alphanumerically\n",
    "    Args:\n",
    "        file_list_ordered (list): list of files to be sorted\n",
    "    Return:\n",
    "        sorted_list (list): input list sorted alphanumerically\n",
    "    \"\"\"\n",
    "    def convert(text):\n",
    "        return int(text) if text.isdigit() else text\n",
    "\n",
    "    def alphanum_key(key):\n",
    "        return [convert(c) for c in re.split('([0-9]+)', key)]\n",
    "\n",
    "    sorted_list = sorted(file_list_ordered, key=alphanum_key)\n",
    "\n",
    "    return sorted_list\n",
    "\n",
    "def get_file_list(path, extension=None):\n",
    "    \"\"\"\n",
    "    Build a list of all the files in the provided path\n",
    "    Args:\n",
    "        path (str): path to the directory \n",
    "        extension (str): only return files with this extension\n",
    "    Return:\n",
    "        file_list (list): list of all the files (with the provided extension) sorted alphanumerically\n",
    "    \"\"\"\n",
    "    if extension is None:\n",
    "        file_list = [os.path.join(path, f) for f in os.listdir(path) if os.path.isfile(os.path.join(path, f))]\n",
    "    else:\n",
    "        file_list = [\n",
    "            os.path.join(path, f)\n",
    "            for f in os.listdir(path)\n",
    "            if os.path.isfile(os.path.join(path, f)) and os.path.splitext(f)[1] == extension\n",
    "        ]\n",
    "    file_list = sorted_alphanum(file_list)\n",
    "\n",
    "    return file_list\n",
    "\n",
    "\n",
    "def get_folder_list(path):\n",
    "    \"\"\"\n",
    "    Build a list of all the folders in the provided path\n",
    "    Args:\n",
    "        path (str): path to the directory \n",
    "    Returns:\n",
    "        folder_list (list): list of all the folders sorted alphanumerically\n",
    "    \"\"\"\n",
    "    folder_list = [os.path.join(path, f) for f in os.listdir(path) if os.path.isdir(os.path.join(path, f))]\n",
    "    folder_list = sorted_alphanum(folder_list)\n",
    "    \n",
    "    return folder_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03479f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_consecutive_point_cloud(pc_s, R1, t1, R2, t2):\n",
    "    pc = np.matmul(np.linalg.inv(R2), (np.matmul(R1, pc_s.transpose()) + t1) - t2).transpose()\n",
    "    return pc\n",
    "\n",
    "def get_eigen_features(pc, n_neighbors=48):\n",
    "    \"\"\"\n",
    "    Args: \n",
    "        pc (np array): points of the point cloud [n, 3]\n",
    "        n_neighbors (int): number of neighbors selected\n",
    "    Returns: \n",
    "        pc_feature (np array): features of the point cloud [n, 14]\n",
    "    \"\"\"\n",
    "    \n",
    "    pcd = o3d.geometry.PointCloud()\n",
    "    pcd.points = o3d.utility.Vector3dVector(pc)\n",
    "    pointcloud = PyntCloud.from_instance(\"open3d\", pcd)\n",
    "\n",
    "    neighbors = pointcloud.get_neighbors(k=n_neighbors)\n",
    "    eigenvalues = pointcloud.add_scalar_field(\"eigen_values\", k_neighbors=neighbors)\n",
    "\n",
    "    anisotropy = pointcloud.add_scalar_field(\"anisotropy\", ev=eigenvalues)\n",
    "    curvature = pointcloud.add_scalar_field(\"curvature\", ev=eigenvalues)\n",
    "    eigenentropy = pointcloud.add_scalar_field(\"eigenentropy\", ev=eigenvalues)\n",
    "    eigensum = pointcloud.add_scalar_field(\"eigen_sum\", ev=eigenvalues)\n",
    "    linearity = pointcloud.add_scalar_field(\"linearity\", ev=eigenvalues)\n",
    "    omnivariance = pointcloud.add_scalar_field(\"omnivariance\", ev=eigenvalues)\n",
    "    planarity = pointcloud.add_scalar_field(\"planarity\", ev=eigenvalues)\n",
    "    sphericity = pointcloud.add_scalar_field(\"sphericity\", ev=eigenvalues)\n",
    "\n",
    "    pc_feature = np.asarray(pointcloud.points)\n",
    "\n",
    "    return pc_feature\n",
    "\n",
    "def cal_chamfer_dist(source_pts, target_pts, length):\n",
    "    if(target_pts.size == 0):\n",
    "        return 24 * length * length\n",
    "    source_pcd = o3d.geometry.PointCloud()\n",
    "    source_pcd.points = o3d.utility.Vector3dVector(source_pts[:, 0:3].reshape(-1, 3))\n",
    "    target_pcd = o3d.geometry.PointCloud()\n",
    "    target_pcd.points = o3d.utility.Vector3dVector(target_pts[:, 0:3].reshape(-1, 3))\n",
    "    \n",
    "    source2target_dists = source_pcd.compute_point_cloud_distance(target_pcd)\n",
    "    target2source_dists = target_pcd.compute_point_cloud_distance(source_pcd)\n",
    "    source2target_dists = np.asarray(source2target_dists)\n",
    "    target2source_dists = np.asarray(target2source_dists)\n",
    "    source2target_dists = np.square(source2target_dists)\n",
    "    target2source_dists = np.square(target2source_dists)\n",
    "    chamfer_dist = source2target_dists.mean() + target2source_dists.mean()\n",
    "    \n",
    "    return chamfer_dist\n",
    "\n",
    "def find_neighbors(voxel_center, source_pts, target_pts, length):\n",
    "    source_idx = np.where((source_pts[:,0] > voxel_center[0] - length) & (source_pts[:,0] < voxel_center[0] + length) & \n",
    "                          (source_pts[:,1] > voxel_center[1] - length) & (source_pts[:,1] < voxel_center[1] + length) & \n",
    "                          (source_pts[:,2] > voxel_center[2] - length) & (source_pts[:,2] < voxel_center[2] + length))[0]\n",
    "    target_idx = np.where((target_pts[:,0] > voxel_center[0] - length) & (target_pts[:,0] < voxel_center[0] + length) & \n",
    "                          (target_pts[:,1] > voxel_center[1] - length) & (target_pts[:,1] < voxel_center[1] + length) & \n",
    "                          (target_pts[:,2] > voxel_center[2] - length) & (target_pts[:,2] < voxel_center[2] + length))[0]\n",
    "    \n",
    "    source_pts = source_pts[source_idx, :]\n",
    "    target_pts = target_pts[target_idx, :]\n",
    "    \n",
    "    return source_pts, target_pts, source_idx\n",
    "\n",
    "def chamfer_dist(source_pts, target_pts, length=1):\n",
    "    pcd = o3d.geometry.PointCloud()\n",
    "    pcd.points = o3d.utility.Vector3dVector(source_pts)\n",
    "    voxel_grid = o3d.geometry.VoxelGrid.create_from_point_cloud(pcd, voxel_size=length * 2)\n",
    "    voxels_all = voxel_grid.get_voxels()\n",
    "    \n",
    "    chamfer = np.ones(source_pts.shape[0]) * -1\n",
    "    \n",
    "    for voxel in voxels_all:\n",
    "        voxel_center = voxel_grid.get_voxel_center_coordinate(voxel.grid_index)\n",
    "        source_neighbors_pts, target_neighbors_pts, source_neighbors_idx = find_neighbors(voxel_center, source_pts, target_pts, length)\n",
    "        chamfer[source_neighbors_idx] = cal_chamfer_dist(source_neighbors_pts, target_neighbors_pts, length)\n",
    "    \n",
    "    return chamfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62294fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_feature(eigen_features):\n",
    "    pcd = o3d.geometry.PointCloud()\n",
    "    pcd.points = o3d.utility.Vector3dVector(eigen_features[:,:3])  \n",
    "    pointcloud = PyntCloud.from_instance(\"open3d\", pcd)\n",
    "    neighbors = pointcloud.get_neighbors(k=48)\n",
    "    \n",
    "    pts_fea_expand = utils.index_points(np.expand_dims(eigen_features, axis=0), np.expand_dims(neighbors, axis=0))\n",
    "    pts = pts_fea_expand[..., :3]\n",
    "    eig = pts_fea_expand[..., 6:]\n",
    "    return np.expand_dims(pts[0], axis=0), np.expand_dims(eig[0], axis=0)\n",
    "\n",
    "def calc_feature(pc_temp, pc_bin, pc_gather):\n",
    "    value = np.multiply(pc_temp, pc_bin)\n",
    "    value = np.sum(value, axis=2, keepdims=True)\n",
    "    num = np.sum(pc_bin, axis=2, keepdims=True)\n",
    "    final = np.squeeze(value / num, axis=(2, ))\n",
    "    pc_gather.append(final)\n",
    "    \n",
    "def attribute(eigen_features): \n",
    "    pc_n, pc_temp = read_feature(eigen_features)\n",
    "    pc_n_center = np.expand_dims(pc_n[:, :, 0, :], axis=2)\n",
    "    pc_n_uncentered = pc_n - pc_n_center\n",
    "    pc_temp = np.concatenate((pc_temp,pc_n_uncentered),axis=-1)\n",
    "\n",
    "    pc_idx = []\n",
    "    pc_idx.append(pc_n_uncentered[:, :, :, 0] >= 0)\n",
    "    pc_idx.append(pc_n_uncentered[:, :, :, 0] <= 0)\n",
    "    pc_idx.append(pc_n_uncentered[:, :, :, 1] >= 0)\n",
    "    pc_idx.append(pc_n_uncentered[:, :, :, 1] <= 0)\n",
    "    pc_idx.append(pc_n_uncentered[:, :, :, 2] >= 0)\n",
    "    pc_idx.append(pc_n_uncentered[:, :, :, 2] <= 0)\n",
    "\n",
    "    pc_bin = []\n",
    "    pc_bin.append(np.expand_dims((pc_idx[0] * pc_idx[2] * pc_idx[4]) * 1.0, axis=3))\n",
    "    pc_bin.append(np.expand_dims((pc_idx[0] * pc_idx[2] * pc_idx[5]) * 1.0, axis=3))\n",
    "    pc_bin.append(np.expand_dims((pc_idx[0] * pc_idx[3] * pc_idx[4]) * 1.0, axis=3))\n",
    "    pc_bin.append(np.expand_dims((pc_idx[0] * pc_idx[3] * pc_idx[5]) * 1.0, axis=3))\n",
    "    pc_bin.append(np.expand_dims((pc_idx[1] * pc_idx[2] * pc_idx[4]) * 1.0, axis=3))\n",
    "    pc_bin.append(np.expand_dims((pc_idx[1] * pc_idx[2] * pc_idx[5]) * 1.0, axis=3))\n",
    "    pc_bin.append(np.expand_dims((pc_idx[1] * pc_idx[3] * pc_idx[4]) * 1.0, axis=3))\n",
    "    pc_bin.append(np.expand_dims((pc_idx[1] * pc_idx[3] * pc_idx[5]) * 1.0, axis=3))\n",
    "\n",
    "    pc_gather1 = []\n",
    "    pc_gather2 = []\n",
    "    pc_gather3 = []\n",
    "    pc_gather4 = []\n",
    "    pc_gather5 = []\n",
    "    pc_gather6 = []\n",
    "    pc_gather7 = []\n",
    "    pc_gather8 = []\n",
    "    threads = []\n",
    "    t1 = threading.Thread(target=calc_feature, args=(pc_temp, pc_bin[0], pc_gather1))\n",
    "    threads.append(t1)\n",
    "    t2 = threading.Thread(target=calc_feature, args=(pc_temp, pc_bin[1], pc_gather2))\n",
    "    threads.append(t2)\n",
    "    t3 = threading.Thread(target=calc_feature, args=(pc_temp, pc_bin[2], pc_gather3))\n",
    "    threads.append(t3)\n",
    "    t4 = threading.Thread(target=calc_feature, args=(pc_temp, pc_bin[3], pc_gather4))\n",
    "    threads.append(t4)\n",
    "    t5 = threading.Thread(target=calc_feature, args=(pc_temp, pc_bin[4], pc_gather5))\n",
    "    threads.append(t5)\n",
    "    t6 = threading.Thread(target=calc_feature, args=(pc_temp, pc_bin[5], pc_gather6))\n",
    "    threads.append(t6)\n",
    "    t7 = threading.Thread(target=calc_feature, args=(pc_temp, pc_bin[6], pc_gather7))\n",
    "    threads.append(t7)\n",
    "    t8 = threading.Thread(target=calc_feature, args=(pc_temp, pc_bin[7], pc_gather8))\n",
    "    threads.append(t8)\n",
    "    for t in threads:\n",
    "        t.setDaemon(False)\n",
    "        t.start()\n",
    "    for t in threads:\n",
    "        # if t.isAlive():\n",
    "        t.join()\n",
    "    pc_gather = pc_gather1 + pc_gather2 + pc_gather3 + pc_gather4 + pc_gather5 + pc_gather6 + pc_gather7 + pc_gather8\n",
    "\n",
    "    pc_fea = np.concatenate(pc_gather, axis=2)\n",
    "\n",
    "    return pc_fea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16ab606",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "file_list = get_file_list('\\\\Workspace\\\\SceneFlow\\\\datasets\\\\flying_things_3d\\\\train')\n",
    "file_list = np.array(file_list)\n",
    "file_index = np.random.choice(len(file_list), 196, replace=False)\n",
    "file_list_sample = file_list[np.array(file_index, dtype=int)]\n",
    "\n",
    "for idx, file in enumerate(file_list_sample):\n",
    "    pc_data = np.load(file)\n",
    "    pc_s = pc_data['pc1']\n",
    "    \n",
    "    # Extract eigen features\n",
    "    eigen_features_s = get_eigen_features(pc_s)\n",
    "    \n",
    "    n1 = pc_s.shape[0]\n",
    "    if n1 >= 2048:\n",
    "        sample_idx1 = np.random.choice(n1, 2048, replace=False)\n",
    "    else:\n",
    "        sample_idx1 = np.concatenate((np.arange(n1), np.random.choice(n1, 2048 - n1, replace=True)), axis=-1)\n",
    "        \n",
    "    pc_s = pc_s[sample_idx1, :]\n",
    "    eigen_features_s = eigen_features_s[sample_idx1, :]\n",
    "\n",
    "    np.savez_compressed(os.path.join('\\\\Workspace\\\\SceneFlow\\\\save', 'FlyingThings3D', '{}.npz'.format(str(idx).zfill(6))),\n",
    "                        pc1=pc_s,\n",
    "                        eigen_features_s=eigen_features_s)\n",
    "\n",
    "    print(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667ef5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_DIR = './'\n",
    "threshold = 0.001\n",
    "\n",
    "file_list_train = []\n",
    "file_list = get_file_list('\\\\Workspace\\\\SceneFlow\\\\save\\\\FlyingThings3D')\n",
    "for file in file_list:\n",
    "    file_list_train.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31334e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list_train = np.array(file_list_train)\n",
    "train_data = np.zeros((file_list_train.shape[0], 2048, 3))\n",
    "data_attribute = np.zeros((file_list_train.shape[0], 2048, 88))\n",
    "\n",
    "for i, file in enumerate(file_list_train):\n",
    "    pc_data = np.load(file)\n",
    "    pc_s = pc_data['pc1']\n",
    "    eigen_features_s = pc_data['eigen_features_s']\n",
    "    \n",
    "    attribute_s = attribute(eigen_features_s)\n",
    "    attribute_s = np.squeeze(attribute_s)\n",
    "    \n",
    "    train_data[i, :, :] = pc_s\n",
    "    data_attribute[i, :, :] = attribute_s\n",
    "    \n",
    "print('Data prepared.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80e0112",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = rpointhop.pointhop_train(train_data, n_sample=[36,8,16], threshold=threshold, attribute_hop1=data_attribute)\n",
    "\n",
    "with open(os.path.join(MODEL_DIR, 'params.pkl'), 'wb') as f:\n",
    "    pickle.dump(model, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
